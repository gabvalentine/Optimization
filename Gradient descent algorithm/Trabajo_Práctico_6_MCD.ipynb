{"cells":[{"cell_type":"markdown","metadata":{"id":"GItxU96mTk16"},"source":["# Matemática para Ciencia de los Datos\n","# Trabajo Práctico 6\n","\n","Profesor: Luis Alexánder Calvo Valverde \n","\n","Instituto Tecnológico de Costa Rica, \n","\n","Programa Ciencia de Datos\n","\n","---\n","\n","Fecha de entrega: Lunes 5 de Junio del 2023, a más tardar a las 3:00 pm.\n","\n","Medio de entrega: Por medio del TEC-Digital.\n","\n","Entregables: Un archivo jupyter ( .IPYNB ). \n","\n","Estudiante:\n","1. Gabriel Valentine Fonseca\n","\n"]},{"cell_type":"markdown","metadata":{"id":"P08xxZR5L1yr"},"source":["## Ejercicio 1 (50 puntos)\n","\n","\n","\n","El algoritmo del descenso de gradiente sigue la idea de modificar el punto óptimo estimado de forma iterativa. Para una función en una\n","variable $f\\left(x\\right)$, la estimación del punto óptimo en una iteración $i+1$ está dada por: \n","\n","\\begin{equation}\n","x\\left(t+1\\right)=x\\left(t\\right)+\\alpha f'\\left(x\\left(t\\right)\\right)\n","\\end{equation}\n","\n","donde el coeficiente $\\alpha$ determina el *grado de confianza o velocidad* con la que el proceso de optimización iterativa sigue\n","la dirección de la derivada. Para la optimización de una función multivariable $f\\left(\\overrightarrow{x}\\left(t\\right)\\right)$ con $\\overrightarrow{x}\\in\\mathbb{R}^{n}$, la posición óptima se estima usando el vector gradiente:\n","\n","\\begin{equation}\n","\\overrightarrow{x}\\left(t+1\\right)=\\overrightarrow{x}\\left(t\\right)+\\alpha\\nabla_{\\overrightarrow{x}}f\\left(\\overrightarrow{x}\\left(t\\right)\\right)\n","\\end{equation}\n","\n","Para la función: \n","\n","\\begin{equation}\n","f\\left(\\overrightarrow{x}\\right)=x^{2}-y^{2},\n","\\end{equation}\n","\n","Implemente la función en python denominada:\n","\n","$$funcion\\_SGD \\left(tasa\\_aprendizaje, iteraciones, xy, tolerancia\\right)$$\n","\n","donde los parámetros corresponden a:\n","\n","* tasa_aprendizaje: es el $\\alpha$\n","* iteraciones: es el máximo número de iteraciones a ejecutar\n","* xy: es el vector con los dos valores iniciales [x,y]\n","* tolerancia: es el valor mínimo para un cambio entre iteración. Si la función de costo no mejora en al menos \"tolerancia\", sale del ciclo de iteración.\n","\n","**Nota:** \n","1. Para iniciar la implementación puede utilizar el código en el cuaderno \"070_1_LACV_Optimizacion\".\n","1. Cada iteración le generará un vector con dos valores ($\\overrightarrow{x}\\left(t+1\\right)$), por lo que para saber el valor de la función de pérdida en ese punto, evalúelo en la función inicial ($x^{2}-y^{2}$) para saber si aumentó o disminuyó.\n","\n","\n","\n","---\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3QcRx2sHL1ys"},"outputs":[],"source":["%matplotlib inline\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from matplotlib import cm\n","import plotly.graph_objects as go\n","\n","# Estilo del gráfico, existe classic\n","plt.style.use('ggplot')"]},{"cell_type":"code","source":["def funcion(x, y):\n","  return x**2 - y**2\n","\n","#Se define la funcion de la derivadad de f de x\n","def dxdf(x,y):\n","    return 2*x\n","\n","#Se define la funcion de la derivadad de f de y\n","def dydf(x,y):\n","    return -2*y\n","\n","#función SGD\n","def funcion_SGD(learning_rate, iters, xy, tolerance):\n","    current_iter = 0\n","    previous_step_size = 1\n","    cur_x = xy\n","    \n","    while previous_step_size > tolerance and current_iter < iters:\n","      prev_x = cur_x #Store current x value in prev_x\n","      gradient = np.array([dxdf(prev_x[0], prev_x[1]), dydf(prev_x[0], prev_x[1])])\n","      cur_x = cur_x - learning_rate * gradient #Grad descent\n","      previous_step_size = sum(abs(cur_x - prev_x)) #Change in x\n","      current_iter = current_iter + 1 #iteration count\n","      if (current_iter % 40) == 0:\n","        print(\"Iteration\",current_iter,\"\\nX value is\",cur_x) #Print iterations every 40\n","  \n","    print(\"\\nThe local minimum occurs at\", cur_x)\n","    print(\"f:\", funcion(cur_x[0], cur_x[1]))\n"],"metadata":{"id":"LJD9YlThU6lw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["funcion_SGD(0.01, 1000, ([3, -3]), 0.000001)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWVXQU9zkO4G","executionInfo":{"status":"ok","timestamp":1685633938954,"user_tz":360,"elapsed":182,"user":{"displayName":"Gabriel Valentine Fonseca","userId":"11743450691920802336"}},"outputId":"5f9acd82-fad4-42fb-d0dd-32357dbc9eb5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 40 \n","X value is [ 1.33710121 -6.62411899]\n","Iteration 80 \n","X value is [  0.59594655 -14.62631747]\n","Iteration 120 \n","X value is [  0.26561362 -32.2954891 ]\n","Iteration 160 \n","X value is [  0.1183841  -71.30972089]\n","Iteration 200 \n","X value is [ 5.27638398e-02 -1.57454692e+02]\n","Iteration 240 \n","X value is [ 2.35168647e-02 -3.47666205e+02]\n","Iteration 280 \n","X value is [ 1.04814761e-02 -7.67660771e+02]\n","Iteration 320 \n","X value is [ 4.67159813e-03 -1.69502543e+03]\n","Iteration 360 \n","X value is [ 2.08213318e-03 -3.74268338e+03]\n","Iteration 400 \n","X value is [ 9.28007597e-04 -8.26399336e+03]\n","Iteration 440 \n","X value is [ 4.13613361e-04 -1.82472251e+04]\n","Iteration 480 \n","X value is [ 1.84347642e-04 -4.02905968e+04]\n","Iteration 520 \n","X value is [ 8.21638185e-05 -8.89632358e+04]\n","Iteration 560 \n","X value is [ 3.66204471e-05 -1.96434353e+05]\n","Iteration 600 \n","X value is [ 1.63217481e-05 -4.33734843e+05]\n","Iteration 640 \n","X value is [ 7.27460971e-06 -9.57703738e+05]\n","Iteration 680 \n","X value is [ 3.24229649e-06 -2.11464784e+06]\n","Iteration 720 \n","X value is [ 1.44509285e-06 -4.66922630e+06]\n","Iteration 760 \n","X value is [ 6.44078469e-07 -1.03098369e+07]\n","Iteration 800 \n","X value is [ 2.87066034e-07 -2.27645287e+07]\n","Iteration 840 \n","X value is [ 1.27945447e-07 -5.02649824e+07]\n","Iteration 880 \n","X value is [ 5.70253375e-08 -1.10987075e+08]\n","Iteration 920 \n","X value is [ 2.54162159e-08 -2.45063863e+08]\n","Iteration 960 \n","X value is [ 1.13280177e-08 -5.41110730e+08]\n","Iteration 1000 \n","X value is [ 5.04890207e-09 -1.19479395e+09]\n","\n","The local minimum occurs at [ 5.04890207e-09 -1.19479395e+09]\n","f: -1.4275325948433441e+18\n"]}]},{"cell_type":"markdown","metadata":{"id":"K00Jx8MuL1ys"},"source":["\n"]},{"cell_type":"markdown","metadata":{"id":"R3OeZxaRL1ys"},"source":["## Ejercicio 2\n","\n","Para la función  $f_{1}\\left(x_{1},x_{2}\\right)=x_{1}^4 + x_{2}^4$\n","\n","Realice lo siguiente:\n","\n","1. En una celda de texto:\n","\n"," - Calcule el vector gradiente. **(15 puntos)**\n","\n"," - Calcule la matriz Hessiana. **(15 puntos)**\n","\n","2. Para el resultado obtenido en el punto anterior: **(20 puntos)**\n","  - Evalúela en el punto $x_{1},x_{2}\\in\\left[4,4\\right]$. \n","  - Luego aplique el criterio de la segunda derivada parcial ¿qué conclusiones saca para ese punto? \n","\n","---"]},{"cell_type":"markdown","source":["Vector gradiente:\n","\n","$\\nabla f_1(x_1, x_2) = \\left(\\frac{\\partial f_1}{\\partial x_1}, \\frac{\\partial f_1}{\\partial x_2}\\right)$\n","\n","Calculamos las derivadas parciales:\n","\n","$\\frac{\\partial f_1}{\\partial x_1} = 4x_1^3$\n","\n","$\\frac{\\partial f_1}{\\partial x_2} = 4x_2^3$\n","\n","Por lo tanto, el vector gradiente es:\n","\n","$\\nabla f_1(x_1, x_2) = \\left(4x_1^3, 4x_2^3\\right)$\n","\n","Su matriz Hessiana:\n","\n","$H_f(x_1, x_2) = \\begin{bmatrix} \\frac{\\partial^2 f_1}{\\partial x_1^2} & \\frac{\\partial^2 f_1}{\\partial x_1 \\partial x_2} \\ \\frac{\\partial^2 f_1}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f_1}{\\partial x_2^2} \\end{bmatrix}$\n","\n","Segundas derivadas parciales:\n","\n","$\\frac{\\partial^2 f_1}{\\partial x_1^2} = 12x_1^2$\n","\n","$\\frac{\\partial^2 f_1}{\\partial x_2^2} = 12x_2^2$\n","\n","$\\frac{\\partial^2 f_1}{\\partial x_1 \\partial x_2} = 0$\n","\n","Por lo tanto, la matriz Hessiana es:\n","\n","$H_f(x_1, x_2) = \\begin{bmatrix} 12x_1^2 & 0 \\\\ 0 & 12x_2^2  \\end{bmatrix}$\n","\n","Para evaluar la matriz Hessiana en el punto $x_1 = 4$, $x_2 = 4$:\n","\n","$H_f(4, 4) = \\begin{bmatrix} 12(4)^2 & 0 \\\\ 0 & 12(4)^2 \\end{bmatrix} = \\begin{bmatrix} 192 & 0 \\\\ 0 & 192 \\end{bmatrix}$\n","\n","Por lo tanto, podemos afirmar que en el punto $(x_1, x_2) = (4, 4)$ la función $f_1(x_1, x_2) = x_1^4 + x_2^4$ tiene un mínimo local."],"metadata":{"id":"1AJo5SsJs_ux"}},{"cell_type":"code","source":[],"metadata":{"id":"e8LEjilZP5uy"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}